============================= test session starts ==============================
platform darwin -- Python 3.13.3, pytest-8.4.2, pluggy-1.6.0 -- /Users/chenxiangting/Workspace/crewAI-examples/crewai_env/bin/python3.13
cachedir: .pytest_cache
rootdir: /Users/chenxiangting/Workspace/crewAI-examples/crews/agentbay_sdk
configfile: pyproject.toml
plugins: anyio-4.11.0, langsmith-0.4.37
collecting ... [92m11:22:59 - LiteLLM:DEBUG[0m: http_handler.py:579 - Using AiohttpTransport...

----------------------------- live log collection ------------------------------
DEBUG    LiteLLM:http_handler.py:579 Using AiohttpTransport...
[92m11:22:59 - LiteLLM:DEBUG[0m: http_handler.py:636 - Creating AiohttpTransport...
DEBUG    LiteLLM:http_handler.py:636 Creating AiohttpTransport...
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10abdf8c0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x10ad162a0> server_hostname='raw.githubusercontent.com' timeout=5
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10ad90190>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'GET']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'GET']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'GET']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'44640'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"cce4f15b9847a53359cc944809f6813d4dad21fc66c88b52abace900217a067a"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'1014:FC916:2902BD:655315:68F6DC5F'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Tue, 21 Oct 2025 03:22:59 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-nrt-rjtf7700052-NRT'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'17'), (b'X-Timer', b'S1761016980.926908,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'28bf9f8aebd4867cbf306e5aa7b73841302da99d'), (b'Expires', b'Tue, 21 Oct 2025 03:27:59 GMT'), (b'Source-Age', b'136')])
INFO     httpx:_client.py:1025 HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'GET']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
[92m11:23:02 - LiteLLM:DEBUG[0m: litellm_logging.py:180 - [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
DEBUG    LiteLLM:litellm_logging.py:180 [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
[92m11:23:02 - LiteLLM:DEBUG[0m: transformation.py:17 - [Non-Blocking] Unable to import _ENTERPRISE_ResponsesSessionHandler - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
DEBUG    LiteLLM:transformation.py:17 [Non-Blocking] Unable to import _ENTERPRISE_ResponsesSessionHandler - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
[92m11:23:02 - LiteLLM:DEBUG[0m: http_handler.py:579 - Using AiohttpTransport...
DEBUG    LiteLLM:http_handler.py:579 Using AiohttpTransport...
[92m11:23:02 - LiteLLM:DEBUG[0m: http_handler.py:636 - Creating AiohttpTransport...
DEBUG    LiteLLM:http_handler.py:636 Creating AiohttpTransport...
[92m11:23:02 - LiteLLM:DEBUG[0m: http_handler.py:579 - Using AiohttpTransport...
DEBUG    LiteLLM:http_handler.py:579 Using AiohttpTransport...
[92m11:23:02 - LiteLLM:DEBUG[0m: http_handler.py:636 - Creating AiohttpTransport...
DEBUG    LiteLLM:http_handler.py:636 Creating AiohttpTransport...
[92m11:23:02 - LiteLLM:DEBUG[0m: http_handler.py:579 - Using AiohttpTransport...
DEBUG    LiteLLM:http_handler.py:579 Using AiohttpTransport...
[92m11:23:02 - LiteLLM:DEBUG[0m: http_handler.py:636 - Creating AiohttpTransport...
DEBUG    LiteLLM:http_handler.py:636 Creating AiohttpTransport...
collected 1 item

src/agentbay_sdk/tests/test_agentbay_code_flow.py::test_run_python_code_flow â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Crew Execution Started â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                                                                                                                                                                                                    â”‚
â”‚  Crew Execution Started                                                                                                                                                                                            â”‚
â”‚  Name: crew                                                                                                                                                                                                        â”‚
â”‚  ID: 4fcd4c94-a9ba-46bf-b4d6-d48be869346c                                                                                                                                                                          â”‚
â”‚  Tool Args:                                                                                                                                                                                                        â”‚
â”‚                                                                                                                                                                                                                    â”‚
â”‚                                                                                                                                                                                                                    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

ðŸš€ Crew: crew
â””â”€â”€ ðŸ“‹ Task: run_code_task (ID: 61d18051-43cc-4f41-adaf-d58061d2a59b)
    Status: Executing Task...â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ðŸ¤– Agent Started â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                                                                                                                                                                                                    â”‚
â”‚  Agent: Cloud Code Execution Agent                                                                                                                                                                                 â”‚
â”‚                                                                                                                                                                                                                    â”‚
â”‚  Task: """ Run a python code snippet in cloud AgentBay session and return the output. You should only return the execution result text without additional explanations.                                            â”‚
â”‚  Code: ---------- print('hello from agentbay') """                                                                                                                                                                 â”‚
â”‚                                                                                                                                                                                                                    â”‚
â”‚                                                                                                                                                                                                                    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

[92m11:23:03 - LiteLLM:DEBUG[0m: utils.py:349 - 


-------------------------------- live log call ---------------------------------
DEBUG    LiteLLM:utils.py:349 

[92m11:23:03 - LiteLLM:DEBUG[0m: utils.py:349 - [92mRequest to litellm:[0m
DEBUG    LiteLLM:utils.py:349 [92mRequest to litellm:[0m
[92m11:23:03 - LiteLLM:DEBUG[0m: utils.py:349 - [92mlitellm.completion(model='openai/qwen-plus', messages=[{'role': 'system', 'content': 'You are Cloud Code Execution Agent. Experienced with remote sessions and secure sandboxes, capable of reliably executing scripts and reporting output.\nYour personal goal is: Execute given code in a secure, controlled cloud environment and return results\nYou ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n\nTool Name: agentbay_run_code\nTool Arguments: {\'code\': {\'description\': \'Code content to execute\', \'type\': \'str\'}, \'language\': {\'description\': \'Programming language, either python or javascript\', \'type\': \'str\'}, \'timeout_s\': {\'description\': \'Timeout in seconds, max 60s\', \'type\': \'int\'}, \'labels\': {\'description\': \'Optional: session labels\', \'type\': \'Union[dict[str, str], NoneType]\'}}\nTool Description: Execute code (python/javascript) in AgentBay cloud session and return stdout text. Input should include: code (the code to execute), language (python or javascript, default: python), timeout_s (execution timeout in seconds, default: 60), and optional labels (dict).\n\nIMPORTANT: Use the following format in your response:\n\n```\nThought: you should always think about what to do\nAction: the action to take, only one name of [agentbay_run_code], just the name, exactly as it\'s written.\nAction Input: the input to the action, just a simple JSON object, enclosed in curly braces, using " to wrap keys and values.\nObservation: the result of the action\n```\n\nOnce all necessary information is gathered, return the following format:\n\n```\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n```'}, {'role': 'user', 'content': '\nCurrent Task: """ Run a python code snippet in cloud AgentBay session and return the output. You should only return the execution result text without additional explanations.\nCode: ---------- print(\'hello from agentbay\') """\n\n\nThis is the expected criteria for your final answer: """ Plain text execution result (stdout), or error message if an exception occurred. """\n\nyou MUST return the actual complete content as the final answer, not a summary.\n\nBegin! This is VERY important to you, use the tools available and give your best Final Answer, your job depends on it!\n\nThought:'}], stop=['\nObservation:'], api_base='https://dashscope.aliyuncs.com/compatible-mode/v1', base_url='https://dashscope.aliyuncs.com/compatible-mode/v1', api_key='sk-5767c27c7c41401ab7556f8c06e7ea50', stream=False)[0m
DEBUG    LiteLLM:utils.py:349 [92mlitellm.completion(model='openai/qwen-plus', messages=[{'role': 'system', 'content': 'You are Cloud Code Execution Agent. Experienced with remote sessions and secure sandboxes, capable of reliably executing scripts and reporting output.\nYour personal goal is: Execute given code in a secure, controlled cloud environment and return results\nYou ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n\nTool Name: agentbay_run_code\nTool Arguments: {\'code\': {\'description\': \'Code content to execute\', \'type\': \'str\'}, \'language\': {\'description\': \'Programming language, either python or javascript\', \'type\': \'str\'}, \'timeout_s\': {\'description\': \'Timeout in seconds, max 60s\', \'type\': \'int\'}, \'labels\': {\'description\': \'Optional: session labels\', \'type\': \'Union[dict[str, str], NoneType]\'}}\nTool Description: Execute code (python/javascript) in AgentBay cloud session and return stdout text. Input should include: code (the code to execute), language (python or javascript, default: python), timeout_s (execution timeout in seconds, default: 60), and optional labels (dict).\n\nIMPORTANT: Use the following format in your response:\n\n```\nThought: you should always think about what to do\nAction: the action to take, only one name of [agentbay_run_code], just the name, exactly as it\'s written.\nAction Input: the input to the action, just a simple JSON object, enclosed in curly braces, using " to wrap keys and values.\nObservation: the result of the action\n```\n\nOnce all necessary information is gathered, return the following format:\n\n```\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n```'}, {'role': 'user', 'content': '\nCurrent Task: """ Run a python code snippet in cloud AgentBay session and return the output. You should only return the execution result text without additional explanations.\nCode: ---------- print(\'hello from agentbay\') """\n\n\nThis is the expected criteria for your final answer: """ Plain text execution result (stdout), or error message if an exception occurred. """\n\nyou MUST return the actual complete content as the final answer, not a summary.\n\nBegin! This is VERY important to you, use the tools available and give your best Final Answer, your job depends on it!\n\nThought:'}], stop=['\nObservation:'], api_base='https://dashscope.aliyuncs.com/compatible-mode/v1', base_url='https://dashscope.aliyuncs.com/compatible-mode/v1', api_key='sk-5767c27c7c41401ab7556f8c06e7ea50', stream=False)[0m
[92m11:23:03 - LiteLLM:DEBUG[0m: utils.py:349 - 

DEBUG    LiteLLM:utils.py:349 

[92m11:23:03 - LiteLLM:DEBUG[0m: utils.py:349 - Initialized litellm callbacks, Async Success Callbacks: [<crewai.utilities.token_counter_callback.TokenCalcHandler object at 0x11c14e8b0>]
DEBUG    LiteLLM:utils.py:349 Initialized litellm callbacks, Async Success Callbacks: [<crewai.utilities.token_counter_callback.TokenCalcHandler object at 0x11c14e8b0>]
[92m11:23:03 - LiteLLM:DEBUG[0m: litellm_logging.py:475 - self.optional_params: {}
DEBUG    LiteLLM:litellm_logging.py:475 self.optional_params: {}
[92m11:23:03 - LiteLLM:DEBUG[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
DEBUG    LiteLLM:utils.py:349 SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
[92m11:23:03 - LiteLLM:INFO[0m: utils.py:3258 - 
LiteLLM completion() model= qwen-plus; provider = openai
INFO     LiteLLM:utils.py:3258 
LiteLLM completion() model= qwen-plus; provider = openai
[92m11:23:03 - LiteLLM:DEBUG[0m: utils.py:3261 - 
LiteLLM: Params passed to completion() {'model': 'qwen-plus', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': ['\nObservation:'], 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'openai', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'You are Cloud Code Execution Agent. Experienced with remote sessions and secure sandboxes, capable of reliably executing scripts and reporting output.\nYour personal goal is: Execute given code in a secure, controlled cloud environment and return results\nYou ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n\nTool Name: agentbay_run_code\nTool Arguments: {\'code\': {\'description\': \'Code content to execute\', \'type\': \'str\'}, \'language\': {\'description\': \'Programming language, either python or javascript\', \'type\': \'str\'}, \'timeout_s\': {\'description\': \'Timeout in seconds, max 60s\', \'type\': \'int\'}, \'labels\': {\'description\': \'Optional: session labels\', \'type\': \'Union[dict[str, str], NoneType]\'}}\nTool Description: Execute code (python/javascript) in AgentBay cloud session and return stdout text. Input should include: code (the code to execute), language (python or javascript, default: python), timeout_s (execution timeout in seconds, default: 60), and optional labels (dict).\n\nIMPORTANT: Use the following format in your response:\n\n```\nThought: you should always think about what to do\nAction: the action to take, only one name of [agentbay_run_code], just the name, exactly as it\'s written.\nAction Input: the input to the action, just a simple JSON object, enclosed in curly braces, using " to wrap keys and values.\nObservation: the result of the action\n```\n\nOnce all necessary information is gathered, return the following format:\n\n```\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n```'}, {'role': 'user', 'content': '\nCurrent Task: """ Run a python code snippet in cloud AgentBay session and return the output. You should only return the execution result text without additional explanations.\nCode: ---------- print(\'hello from agentbay\') """\n\n\nThis is the expected criteria for your final answer: """ Plain text execution result (stdout), or error message if an exception occurred. """\n\nyou MUST return the actual complete content as the final answer, not a summary.\n\nBegin! This is VERY important to you, use the tools available and give your best Final Answer, your job depends on it!\n\nThought:'}], 'thinking': None, 'web_search_options': None}
DEBUG    LiteLLM:utils.py:3261 
LiteLLM: Params passed to completion() {'model': 'qwen-plus', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': ['\nObservation:'], 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'openai', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'You are Cloud Code Execution Agent. Experienced with remote sessions and secure sandboxes, capable of reliably executing scripts and reporting output.\nYour personal goal is: Execute given code in a secure, controlled cloud environment and return results\nYou ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n\nTool Name: agentbay_run_code\nTool Arguments: {\'code\': {\'description\': \'Code content to execute\', \'type\': \'str\'}, \'language\': {\'description\': \'Programming language, either python or javascript\', \'type\': \'str\'}, \'timeout_s\': {\'description\': \'Timeout in seconds, max 60s\', \'type\': \'int\'}, \'labels\': {\'description\': \'Optional: session labels\', \'type\': \'Union[dict[str, str], NoneType]\'}}\nTool Description: Execute code (python/javascript) in AgentBay cloud session and return stdout text. Input should include: code (the code to execute), language (python or javascript, default: python), timeout_s (execution timeout in seconds, default: 60), and optional labels (dict).\n\nIMPORTANT: Use the following format in your response:\n\n```\nThought: you should always think about what to do\nAction: the action to take, only one name of [agentbay_run_code], just the name, exactly as it\'s written.\nAction Input: the input to the action, just a simple JSON object, enclosed in curly braces, using " to wrap keys and values.\nObservation: the result of the action\n```\n\nOnce all necessary information is gathered, return the following format:\n\n```\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n```'}, {'role': 'user', 'content': '\nCurrent Task: """ Run a python code snippet in cloud AgentBay session and return the output. You should only return the execution result text without additional explanations.\nCode: ---------- print(\'hello from agentbay\') """\n\n\nThis is the expected criteria for your final answer: """ Plain text execution result (stdout), or error message if an exception occurred. """\n\nyou MUST return the actual complete content as the final answer, not a summary.\n\nBegin! This is VERY important to you, use the tools available and give your best Final Answer, your job depends on it!\n\nThought:'}], 'thinking': None, 'web_search_options': None}
[92m11:23:03 - LiteLLM:DEBUG[0m: utils.py:3264 - 
LiteLLM: Non-Default params passed to completion() {'stream': False, 'stop': ['\nObservation:']}
DEBUG    LiteLLM:utils.py:3264 
LiteLLM: Non-Default params passed to completion() {'stream': False, 'stop': ['\nObservation:']}
[92m11:23:03 - LiteLLM:DEBUG[0m: utils.py:349 - Final returned optional params: {'stream': False, 'stop': ['\nObservation:'], 'extra_body': {}}
DEBUG    LiteLLM:utils.py:349 Final returned optional params: {'stream': False, 'stop': ['\nObservation:'], 'extra_body': {}}
[92m11:23:03 - LiteLLM:DEBUG[0m: litellm_logging.py:475 - self.optional_params: {'stream': False, 'stop': ['\nObservation:'], 'extra_body': {}}
DEBUG    LiteLLM:litellm_logging.py:475 self.optional_params: {'stream': False, 'stop': ['\nObservation:'], 'extra_body': {}}
[92m11:23:03 - LiteLLM:DEBUG[0m: utils.py:4620 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
DEBUG    LiteLLM:utils.py:4620 checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
[92m11:23:03 - LiteLLM:DEBUG[0m: utils.py:4835 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
DEBUG    LiteLLM:utils.py:4835 Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m11:23:03 - LiteLLM:DEBUG[0m: main.py:858 - Error getting model info: This model isn't mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
DEBUG    LiteLLM:main.py:858 Error getting model info: This model isn't mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
[92m11:23:03 - LiteLLM:DEBUG[0m: litellm_logging.py:923 - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://dashscope.aliyuncs.com/compatible-mode/v1/ \
-d '{'model': 'qwen-plus', 'messages': [{'role': 'system', 'content': 'You are Cloud Code Execution Agent. Experienced with remote sessions and secure sandboxes, capable of reliably executing scripts and reporting output.\nYour personal goal is: Execute given code in a secure, controlled cloud environment and return results\nYou ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n\nTool Name: agentbay_run_code\nTool Arguments: {\'code\': {\'description\': \'Code content to execute\', \'type\': \'str\'}, \'language\': {\'description\': \'Programming language, either python or javascript\', \'type\': \'str\'}, \'timeout_s\': {\'description\': \'Timeout in seconds, max 60s\', \'type\': \'int\'}, \'labels\': {\'description\': \'Optional: session labels\', \'type\': \'Union[dict[str, str], NoneType]\'}}\nTool Description: Execute code (python/javascript) in AgentBay cloud session and return stdout text. Input should include: code (the code to execute), language (python or javascript, default: python), timeout_s (execution timeout in seconds, default: 60), and optional labels (dict).\n\nIMPORTANT: Use the following format in your response:\n\n```\nThought: you should always think about what to do\nAction: the action to take, only one name of [agentbay_run_code], just the name, exactly as it\'s written.\nAction Input: the input to the action, just a simple JSON object, enclosed in curly braces, using " to wrap keys and values.\nObservation: the result of the action\n```\n\nOnce all necessary information is gathered, return the following format:\n\n```\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n```'}, {'role': 'user', 'content': '\nCurrent Task: """ Run a python code snippet in cloud AgentBay session and return the output. You should only return the execution result text without additional explanations.\nCode: ---------- print(\'hello from agentbay\') """\n\n\nThis is the expected criteria for your final answer: """ Plain text execution result (stdout), or error message if an exception occurred. """\n\nyou MUST return the actual complete content as the final answer, not a summary.\n\nBegin! This is VERY important to you, use the tools available and give your best Final Answer, your job depends on it!\n\nThought:'}], 'stop': ['\nObservation:'], 'extra_body': {}}'
[0m

DEBUG    LiteLLM:litellm_logging.py:923 [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://dashscope.aliyuncs.com/compatible-mode/v1/ \
-d '{'model': 'qwen-plus', 'messages': [{'role': 'system', 'content': 'You are Cloud Code Execution Agent. Experienced with remote sessions and secure sandboxes, capable of reliably executing scripts and reporting output.\nYour personal goal is: Execute given code in a secure, controlled cloud environment and return results\nYou ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n\nTool Name: agentbay_run_code\nTool Arguments: {\'code\': {\'description\': \'Code content to execute\', \'type\': \'str\'}, \'language\': {\'description\': \'Programming language, either python or javascript\', \'type\': \'str\'}, \'timeout_s\': {\'description\': \'Timeout in seconds, max 60s\', \'type\': \'int\'}, \'labels\': {\'description\': \'Optional: session labels\', \'type\': \'Union[dict[str, str], NoneType]\'}}\nTool Description: Execute code (python/javascript) in AgentBay cloud session and return stdout text. Input should include: code (the code to execute), language (python or javascript, default: python), timeout_s (execution timeout in seconds, default: 60), and optional labels (dict).\n\nIMPORTANT: Use the following format in your response:\n\n```\nThought: you should always think about what to do\nAction: the action to take, only one name of [agentbay_run_code], just the name, exactly as it\'s written.\nAction Input: the input to the action, just a simple JSON object, enclosed in curly braces, using " to wrap keys and values.\nObservation: the result of the action\n```\n\nOnce all necessary information is gathered, return the following format:\n\n```\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n```'}, {'role': 'user', 'content': '\nCurrent Task: """ Run a python code snippet in cloud AgentBay session and return the output. You should only return the execution result text without additional explanations.\nCode: ---------- print(\'hello from agentbay\') """\n\n\nThis is the expected criteria for your final answer: """ Plain text execution result (stdout), or error message if an exception occurred. """\n\nyou MUST return the actual complete content as the final answer, not a summary.\n\nBegin! This is VERY important to you, use the tools available and give your best Final Answer, your job depends on it!\n\nThought:'}], 'stop': ['\nObservation:'], 'extra_body': {}}'
[0m

DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'timeout': 600.0, 'files': None, 'idempotency_key': 'stainless-python-retry-e3bd9ccc-60fe-46dd-8815-b76a97614e28', 'json_data': {'messages': [{'role': 'system', 'content': 'You are Cloud Code Execution Agent. Experienced with remote sessions and secure sandboxes, capable of reliably executing scripts and reporting output.\nYour personal goal is: Execute given code in a secure, controlled cloud environment and return results\nYou ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n\nTool Name: agentbay_run_code\nTool Arguments: {\'code\': {\'description\': \'Code content to execute\', \'type\': \'str\'}, \'language\': {\'description\': \'Programming language, either python or javascript\', \'type\': \'str\'}, \'timeout_s\': {\'description\': \'Timeout in seconds, max 60s\', \'type\': \'int\'}, \'labels\': {\'description\': \'Optional: session labels\', \'type\': \'Union[dict[str, str], NoneType]\'}}\nTool Description: Execute code (python/javascript) in AgentBay cloud session and return stdout text. Input should include: code (the code to execute), language (python or javascript, default: python), timeout_s (execution timeout in seconds, default: 60), and optional labels (dict).\n\nIMPORTANT: Use the following format in your response:\n\n```\nThought: you should always think about what to do\nAction: the action to take, only one name of [agentbay_run_code], just the name, exactly as it\'s written.\nAction Input: the input to the action, just a simple JSON object, enclosed in curly braces, using " to wrap keys and values.\nObservation: the result of the action\n```\n\nOnce all necessary information is gathered, return the following format:\n\n```\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n```'}, {'role': 'user', 'content': '\nCurrent Task: """ Run a python code snippet in cloud AgentBay session and return the output. You should only return the execution result text without additional explanations.\nCode: ---------- print(\'hello from agentbay\') """\n\n\nThis is the expected criteria for your final answer: """ Plain text execution result (stdout), or error message if an exception occurred. """\n\nyou MUST return the actual complete content as the final answer, not a summary.\n\nBegin! This is VERY important to you, use the tools available and give your best Final Answer, your job depends on it!\n\nThought:'}], 'model': 'qwen-plus', 'stop': ['\nObservation:']}, 'extra_json': {}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='dashscope.aliyuncs.com' port=443 local_address=None timeout=600.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11c79dbd0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x11c133920> server_hostname='dashscope.aliyuncs.com' timeout=600.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11c2f6520>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'691ab756-928d-4243-b98c-b6d3a1550931'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'1242'), (b'req-arrive-time', b'1761016983383'), (b'resp-start-time', b'1761016984625'), (b'x-envoy-upstream-service-time', b'1240'), (b'set-cookie', b'acw_tc=691ab756-928d-4243-b98c-b6d3a155093156474b5d6c099cc124fc8efbc44784f7;path=/;HttpOnly;Max-Age=1800'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 21 Oct 2025 03:23:04 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '691ab756-928d-4243-b98c-b6d3a1550931', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '1242', 'req-arrive-time': '1761016983383', 'resp-start-time': '1761016984625', 'x-envoy-upstream-service-time': '1240', 'set-cookie': 'acw_tc=691ab756-928d-4243-b98c-b6d3a155093156474b5d6c099cc124fc8efbc44784f7;path=/;HttpOnly;Max-Age=1800', 'content-encoding': 'gzip', 'date': 'Tue, 21 Oct 2025 03:23:04 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 691ab756-928d-4243-b98c-b6d3a1550931
[92m11:23:04 - LiteLLM:DEBUG[0m: utils.py:349 - RAW RESPONSE:
{"id": "chatcmpl-691ab756-928d-4243-b98c-b6d3a1550931", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "Thought: I need to execute the provided Python code snippet in the cloud AgentBay session.\nAction: agentbay_run_code\nAction Input: {\"code\": \"print('hello from agentbay')\", \"language\": \"python\", \"timeout_s\": 60}", "refusal": null, "role": "assistant", "annotations": null, "audio": null, "function_call": null, "tool_calls": null}}], "created": 1761016985, "model": "qwen-plus", "object": "chat.completion", "service_tier": null, "system_fingerprint": null, "usage": {"completion_tokens": 54, "prompt_tokens": 478, "total_tokens": 532, "completion_tokens_details": null, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0}}}


DEBUG    LiteLLM:utils.py:349 RAW RESPONSE:
{"id": "chatcmpl-691ab756-928d-4243-b98c-b6d3a1550931", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "Thought: I need to execute the provided Python code snippet in the cloud AgentBay session.\nAction: agentbay_run_code\nAction Input: {\"code\": \"print('hello from agentbay')\", \"language\": \"python\", \"timeout_s\": 60}", "refusal": null, "role": "assistant", "annotations": null, "audio": null, "function_call": null, "tool_calls": null}}], "created": 1761016985, "model": "qwen-plus", "object": "chat.completion", "service_tier": null, "system_fingerprint": null, "usage": {"completion_tokens": 54, "prompt_tokens": 478, "total_tokens": 532, "completion_tokens_details": null, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0}}}


[92m11:23:04 - LiteLLM:INFO[0m: utils.py:1260 - Wrapper: Completed Call, calling success_handler
INFO     LiteLLM:utils.py:1260 Wrapper: Completed Call, calling success_handler
[92m11:23:04 - LiteLLM:DEBUG[0m: litellm_logging.py:1595 - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG    LiteLLM:litellm_logging.py:1595 Logging Details LiteLLM-Success Call: Cache_hit=None
[92m11:23:04 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: openai/qwen-plus
DEBUG    LiteLLM:cost_calculator.py:675 selected model name for cost calculation: openai/qwen-plus
[92m11:23:04 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: openai/qwen-plus
DEBUG    LiteLLM:cost_calculator.py:675 selected model name for cost calculation: openai/qwen-plus
[92m11:23:04 - LiteLLM:DEBUG[0m: utils.py:4620 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
[92m11:23:04 - LiteLLM:DEBUG[0m: utils.py:4620 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
DEBUG    LiteLLM:utils.py:4620 checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
[92m11:23:04 - LiteLLM:DEBUG[0m: utils.py:4835 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
DEBUG    LiteLLM:utils.py:4620 checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
[92m11:23:04 - LiteLLM:DEBUG[0m: utils.py:4835 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
DEBUG    LiteLLM:utils.py:4835 Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m11:23:04 - LiteLLM:DEBUG[0m: cost_calculator.py:947 - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/qwen-plus - This model isn't mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
DEBUG    LiteLLM:utils.py:4835 Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m11:23:04 - LiteLLM:DEBUG[0m: cost_calculator.py:947 - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/qwen-plus - This model isn't mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
DEBUG    LiteLLM:cost_calculator.py:947 litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/qwen-plus - This model isn't mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
[92m11:23:04 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: openai/qwen-plus
DEBUG    LiteLLM:cost_calculator.py:947 litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/qwen-plus - This model isn't mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
[92m11:23:04 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: qwen-plus
DEBUG    LiteLLM:cost_calculator.py:675 selected model name for cost calculation: openai/qwen-plus
[92m11:23:04 - LiteLLM:DEBUG[0m: utils.py:4620 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
DEBUG    LiteLLM:cost_calculator.py:675 selected model name for cost calculation: qwen-plus
[92m11:23:04 - LiteLLM:DEBUG[0m: utils.py:4620 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
DEBUG    LiteLLM:utils.py:4620 checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
[92m11:23:04 - LiteLLM:DEBUG[0m: utils.py:4835 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
DEBUG    LiteLLM:utils.py:4620 checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
[92m11:23:04 - LiteLLM:DEBUG[0m: utils.py:4835 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
DEBUG    LiteLLM:utils.py:4835 Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
DEBUG    LiteLLM:utils.py:4835 Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m11:23:04 - LiteLLM:DEBUG[0m: cost_calculator.py:947 - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/qwen-plus - This model isn't mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
DEBUG    LiteLLM:cost_calculator.py:947 litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/qwen-plus - This model isn't mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
[92m11:23:04 - LiteLLM:DEBUG[0m: cost_calculator.py:947 - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=qwen-plus - This model isn't mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
DEBUG    LiteLLM:cost_calculator.py:947 litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=qwen-plus - This model isn't mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
[92m11:23:04 - LiteLLM:DEBUG[0m: litellm_logging.py:1239 - response_cost_failure_debug_information: {'error_str': "This model isn't mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.", 'traceback_str': 'Traceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4709, in _get_model_info_helper\n    raise ValueError(\n        "This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json"\n    )\nValueError: This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 1221, in _response_cost_calculator\n    response_cost = litellm.response_cost_calculator(\n        **response_cost_calculator_kwargs\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1062, in response_cost_calculator\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1046, in response_cost_calculator\n    response_cost = completion_cost(\n        completion_response=response_object,\n    ...<10 lines>...\n        litellm_logging_obj=litellm_logging_obj,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 960, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 953, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 915, in completion_cost\n    ) = cost_per_token(\n        ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<14 lines>...\n        rerank_billed_units=rerank_billed_units,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 335, in cost_per_token\n    return openai_cost_per_token(model=model, usage=usage_block)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/llms/openai/cost_calculation.py", line 33, in cost_per_token\n    return generic_cost_per_token(\n        model=model, usage=usage, custom_llm_provider="openai"\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py", line 207, in generic_cost_per_token\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4920, in get_model_info\n    _model_info = _get_model_info_helper(\n        model=model,\n        custom_llm_provider=custom_llm_provider,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4838, in _get_model_info_helper\n    raise Exception(\n    ...<3 lines>...\n    )\nException: This model isn\'t mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n', 'model': 'qwen-plus', 'cache_hit': None, 'custom_llm_provider': 'openai', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}
DEBUG    LiteLLM:litellm_logging.py:1239 response_cost_failure_debug_information: {'error_str': "This model isn't mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.", 'traceback_str': 'Traceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4709, in _get_model_info_helper\n    raise ValueError(\n        "This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json"\n    )\nValueError: This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 1221, in _response_cost_calculator\n    response_cost = litellm.response_cost_calculator(\n        **response_cost_calculator_kwargs\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1062, in response_cost_calculator\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1046, in response_cost_calculator\n    response_cost = completion_cost(\n        completion_response=response_object,\n    ...<10 lines>...\n        litellm_logging_obj=litellm_logging_obj,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 960, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 953, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 915, in completion_cost\n    ) = cost_per_token(\n        ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<14 lines>...\n        rerank_billed_units=rerank_billed_units,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 335, in cost_per_token\n    return openai_cost_per_token(model=model, usage=usage_block)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/llms/openai/cost_calculation.py", line 33, in cost_per_token\n    return generic_cost_per_token(\n        model=model, usage=usage, custom_llm_provider="openai"\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py", line 207, in generic_cost_per_token\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4920, in get_model_info\n    _model_info = _get_model_info_helper(\n        model=model,\n        custom_llm_provider=custom_llm_provider,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4838, in _get_model_info_helper\n    raise Exception(\n    ...<3 lines>...\n    )\nException: This model isn\'t mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n', 'model': 'qwen-plus', 'cache_hit': None, 'custom_llm_provider': 'openai', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}
[92m11:23:04 - LiteLLM:DEBUG[0m: litellm_logging.py:1239 - response_cost_failure_debug_information: {'error_str': "This model isn't mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.", 'traceback_str': 'Traceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4709, in _get_model_info_helper\n    raise ValueError(\n        "This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json"\n    )\nValueError: This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 1221, in _response_cost_calculator\n    response_cost = litellm.response_cost_calculator(\n        **response_cost_calculator_kwargs\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1062, in response_cost_calculator\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1046, in response_cost_calculator\n    response_cost = completion_cost(\n        completion_response=response_object,\n    ...<10 lines>...\n        litellm_logging_obj=litellm_logging_obj,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 960, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 953, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 915, in completion_cost\n    ) = cost_per_token(\n        ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<14 lines>...\n        rerank_billed_units=rerank_billed_units,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 335, in cost_per_token\n    return openai_cost_per_token(model=model, usage=usage_block)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/llms/openai/cost_calculation.py", line 33, in cost_per_token\n    return generic_cost_per_token(\n        model=model, usage=usage, custom_llm_provider="openai"\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py", line 207, in generic_cost_per_token\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4920, in get_model_info\n    _model_info = _get_model_info_helper(\n        model=model,\n        custom_llm_provider=custom_llm_provider,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4838, in _get_model_info_helper\n    raise Exception(\n    ...<3 lines>...\n    )\nException: This model isn\'t mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n', 'model': 'openai/qwen-plus', 'cache_hit': False, 'custom_llm_provider': 'openai', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}
DEBUG    LiteLLM:litellm_logging.py:1239 response_cost_failure_debug_information: {'error_str': "This model isn't mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.", 'traceback_str': 'Traceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4709, in _get_model_info_helper\n    raise ValueError(\n        "This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json"\n    )\nValueError: This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 1221, in _response_cost_calculator\n    response_cost = litellm.response_cost_calculator(\n        **response_cost_calculator_kwargs\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1062, in response_cost_calculator\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1046, in response_cost_calculator\n    response_cost = completion_cost(\n        completion_response=response_object,\n    ...<10 lines>...\n        litellm_logging_obj=litellm_logging_obj,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 960, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 953, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 915, in completion_cost\n    ) = cost_per_token(\n        ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<14 lines>...\n        rerank_billed_units=rerank_billed_units,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 335, in cost_per_token\n    return openai_cost_per_token(model=model, usage=usage_block)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/llms/openai/cost_calculation.py", line 33, in cost_per_token\n    return generic_cost_per_token(\n        model=model, usage=usage, custom_llm_provider="openai"\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py", line 207, in generic_cost_per_token\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4920, in get_model_info\n    _model_info = _get_model_info_helper(\n        model=model,\n        custom_llm_provider=custom_llm_provider,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4838, in _get_model_info_helper\n    raise Exception(\n    ...<3 lines>...\n    )\nException: This model isn\'t mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n', 'model': 'openai/qwen-plus', 'cache_hit': False, 'custom_llm_provider': 'openai', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}
[92m11:23:04 - LiteLLM:DEBUG[0m: utils.py:4620 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
DEBUG    LiteLLM:utils.py:4620 checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
[92m11:23:04 - LiteLLM:DEBUG[0m: utils.py:4835 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
DEBUG    LiteLLM:utils.py:4835 Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m11:23:04 - LiteLLM:DEBUG[0m: litellm_logging.py:3956 - Model=qwen-plus is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload
DEBUG    LiteLLM:litellm_logging.py:3956 Model=qwen-plus is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload
[92m11:23:04 - LiteLLM:DEBUG[0m: litellm_logging.py:1624 - Logging Details LiteLLM-Success Call streaming complete
DEBUG    LiteLLM:litellm_logging.py:1624 Logging Details LiteLLM-Success Call streaming complete
[92m11:23:04 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: openai/qwen-plus
DEBUG    LiteLLM:cost_calculator.py:675 selected model name for cost calculation: openai/qwen-plus
[92m11:23:04 - LiteLLM:DEBUG[0m: utils.py:4620 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
DEBUG    LiteLLM:utils.py:4620 checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
[92m11:23:04 - LiteLLM:DEBUG[0m: utils.py:4835 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
DEBUG    LiteLLM:utils.py:4835 Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m11:23:04 - LiteLLM:DEBUG[0m: cost_calculator.py:947 - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/qwen-plus - This model isn't mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
DEBUG    LiteLLM:cost_calculator.py:947 litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/qwen-plus - This model isn't mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
[92m11:23:04 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: qwen-plus
DEBUG    LiteLLM:cost_calculator.py:675 selected model name for cost calculation: qwen-plus
[92m11:23:04 - LiteLLM:DEBUG[0m: utils.py:4620 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
DEBUG    LiteLLM:utils.py:4620 checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
[92m11:23:04 - LiteLLM:DEBUG[0m: utils.py:4835 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
DEBUG    LiteLLM:utils.py:4835 Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m11:23:04 - LiteLLM:DEBUG[0m: cost_calculator.py:947 - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=qwen-plus - This model isn't mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
DEBUG    LiteLLM:cost_calculator.py:947 litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=qwen-plus - This model isn't mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
[92m11:23:04 - LiteLLM:DEBUG[0m: litellm_logging.py:1239 - response_cost_failure_debug_information: {'error_str': "This model isn't mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.", 'traceback_str': 'Traceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4709, in _get_model_info_helper\n    raise ValueError(\n        "This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json"\n    )\nValueError: This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 1221, in _response_cost_calculator\n    response_cost = litellm.response_cost_calculator(\n        **response_cost_calculator_kwargs\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1062, in response_cost_calculator\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1046, in response_cost_calculator\n    response_cost = completion_cost(\n        completion_response=response_object,\n    ...<10 lines>...\n        litellm_logging_obj=litellm_logging_obj,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 960, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 953, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 915, in completion_cost\n    ) = cost_per_token(\n        ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<14 lines>...\n        rerank_billed_units=rerank_billed_units,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 335, in cost_per_token\n    return openai_cost_per_token(model=model, usage=usage_block)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/llms/openai/cost_calculation.py", line 33, in cost_per_token\n    return generic_cost_per_token(\n        model=model, usage=usage, custom_llm_provider="openai"\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py", line 207, in generic_cost_per_token\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4920, in get_model_info\n    _model_info = _get_model_info_helper(\n        model=model,\n        custom_llm_provider=custom_llm_provider,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4838, in _get_model_info_helper\n    raise Exception(\n    ...<3 lines>...\n    )\nException: This model isn\'t mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n', 'model': 'qwen-plus', 'cache_hit': None, 'custom_llm_provider': 'openai', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}
DEBUG    LiteLLM:litellm_logging.py:1239 response_cost_failure_debug_information: {'error_str': "This model isn't mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.", 'traceback_str': 'Traceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4709, in _get_model_info_helper\n    raise ValueError(\n        "This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json"\n    )\nValueError: This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 1221, in _response_cost_calculator\n    response_cost = litellm.response_cost_calculator(\n        **response_cost_calculator_kwargs\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1062, in response_cost_calculator\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1046, in response_cost_calculator\n    response_cost = completion_cost(\n        completion_response=response_object,\n    ...<10 lines>...\n        litellm_logging_obj=litellm_logging_obj,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 960, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 953, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 915, in completion_cost\n    ) = cost_per_token(\n        ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<14 lines>...\n        rerank_billed_units=rerank_billed_units,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 335, in cost_per_token\n    return openai_cost_per_token(model=model, usage=usage_block)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/llms/openai/cost_calculation.py", line 33, in cost_per_token\n    return generic_cost_per_token(\n        model=model, usage=usage, custom_llm_provider="openai"\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py", line 207, in generic_cost_per_token\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4920, in get_model_info\n    _model_info = _get_model_info_helper(\n        model=model,\n        custom_llm_provider=custom_llm_provider,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4838, in _get_model_info_helper\n    raise Exception(\n    ...<3 lines>...\n    )\nException: This model isn\'t mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n', 'model': 'qwen-plus', 'cache_hit': None, 'custom_llm_provider': 'openai', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}
[92m11:23:04 - LiteLLM:DEBUG[0m: utils.py:4620 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
DEBUG    LiteLLM:utils.py:4620 checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
[92m11:23:04 - LiteLLM:DEBUG[0m: utils.py:4835 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
DEBUG    LiteLLM:utils.py:4835 Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m11:23:04 - LiteLLM:DEBUG[0m: litellm_logging.py:3956 - Model=qwen-plus is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload
DEBUG    LiteLLM:litellm_logging.py:3956 Model=qwen-plus is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload
[32m2025-10-21 11:23:04.573[0m | [1m[34mAgentBay[0m[1m[0m | [1mINFO[0m | [33m84743:8778162368[0m | [36magentbay.config[0m:[36mload_dotenv_with_fallback[0m:[36m89[0m | [1mLoaded .env file from: /Users/chenxiangting/Workspace/crewAI-examples/crews/agentbay_sdk/.env[0m
[32m2025-10-21 11:23:04.573[0m | [1m[34mAgentBay[0m[1m[0m | [1mINFO[0m | [33m84743:8778162368[0m | [36magentbay.context[0m:[36mget[0m:[36m344[0m | [1mðŸ”— API Call: GetContext[0m
DEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): wuyingai.cn-shanghai.aliyuncs.com:443
DEBUG    urllib3.connectionpool:connectionpool.py:544 https://wuyingai.cn-shanghai.aliyuncs.com:443 "POST /?Action=GetContext&Format=json&Version=2025-05-06&Timestamp=2025-10-21T03%3A23%3A04Z&SignatureNonce=c37908d1daecd03a1e95600fd370ccfb HTTP/1.1" 200 255
[32m2025-10-21 11:23:04.911[0m | [1m[34mAgentBay[0m[1m[0m | [1mINFO[0m | [33m84743:8778162368[0m | [36magentbay.context[0m:[36mget[0m:[36m355[0m | [1mâœ… API Response received[0m
[32m2025-10-21 11:23:04.911[0m | [1m[34mAgentBay[0m[1m[0m | [1mINFO[0m | [33m84743:8778162368[0m | [36magentbay.agentbay[0m:[36mcreate[0m:[36m395[0m | [1mAdding context sync for file transfer operations: ContextSync(context_id='SdkCtx-04b2mq8td7fdvhtl2', path='/temp/file-transfer', policy=None)[0m
DEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): telemetry.crewai.com:4319
DEBUG    urllib3.connectionpool:connectionpool.py:544 https://telemetry.crewai.com:4319 "POST /v1/traces HTTP/1.1" 200 2
DEBUG    urllib3.connectionpool:connectionpool.py:544 https://wuyingai.cn-shanghai.aliyuncs.com:443 "POST /?Action=CreateMcpSession&Format=json&Version=2025-05-06&Timestamp=2025-10-21T03%3A23%3A04Z&SignatureNonce=3aa4b05646a189bb542a214d11ef24cb HTTP/1.1" 200 258
[32m2025-10-21 11:23:08.492[0m | [1m[34mAgentBay[0m[1m[0m | [1mINFO[0m | [33m84743:8778162368[0m | [36magentbay.agentbay[0m:[36mcreate[0m:[36m552[0m | [1mâœ… API Response received[0m
[32m2025-10-21 11:23:08.492[0m | [1m[34mAgentBay[0m[1m[0m | [1mINFO[0m | [33m84743:8778162368[0m | [36magentbay.agentbay[0m:[36m_build_session_from_response[0m:[36m177[0m | [1mðŸ†” Session created: session-04b2mslt4bfnkpvaa[0m
[32m2025-10-21 11:23:08.493[0m | [1m[34mAgentBay[0m[1m[0m | [1mINFO[0m | [33m84743:8778162368[0m | [36magentbay.agentbay[0m:[36m_wait_for_context_synchronization[0m:[36m248[0m | [1mðŸš€ Starting: Context synchronization[0m
[32m2025-10-21 11:23:08.493[0m | [1m[34mAgentBay[0m[1m[0m | [1mINFO[0m | [33m84743:8778162368[0m | [36magentbay.context_manager[0m:[36minfo[0m:[36m89[0m | [1mðŸ”— API Call: GetContextInfo[0m
DEBUG    urllib3.connectionpool:connectionpool.py:544 https://wuyingai.cn-shanghai.aliyuncs.com:443 "POST /?Action=GetContextInfo&Format=json&Version=2025-05-06&Timestamp=2025-10-21T03%3A23%3A08Z&SignatureNonce=57d30607c44e2a731cbae2e425eb58ee HTTP/1.1" 200 414
[32m2025-10-21 11:23:08.713[0m | [1m[34mAgentBay[0m[1m[0m | [1mINFO[0m | [33m84743:8778162368[0m | [36magentbay.context_manager[0m:[36minfo[0m:[36m98[0m | [1mâœ… API Response received[0m
[32m2025-10-21 11:23:08.714[0m | [1m[34mAgentBay[0m[1m[0m | [1mINFO[0m | [33m84743:8778162368[0m | [36magentbay.agentbay[0m:[36m_wait_for_context_synchronization[0m:[36m265[0m | [1mðŸ“ Context SdkCtx-04b2mq8td7fdvhtl2 status: Preparing, path: /temp/file-transfer[0m
[32m2025-10-21 11:23:08.714[0m | [1m[34mAgentBay[0m[1m[0m | [1mINFO[0m | [33m84743:8778162368[0m | [36magentbay.agentbay[0m:[36m_wait_for_context_synchronization[0m:[36m286[0m | [1mâ³ Waiting for context synchronization, attempt 1/150[0m
[32m2025-10-21 11:23:10.718[0m | [1m[34mAgentBay[0m[1m[0m | [1mINFO[0m | [33m84743:8778162368[0m | [36magentbay.context_manager[0m:[36minfo[0m:[36m89[0m | [1mðŸ”— API Call: GetContextInfo[0m
DEBUG    urllib3.connectionpool:connectionpool.py:544 https://wuyingai.cn-shanghai.aliyuncs.com:443 "POST /?Action=GetContextInfo&Format=json&Version=2025-05-06&Timestamp=2025-10-21T03%3A23%3A10Z&SignatureNonce=498aea3a075f43d579243e4f6a84fcbc HTTP/1.1" 200 412
[32m2025-10-21 11:23:10.927[0m | [1m[34mAgentBay[0m[1m[0m | [1mINFO[0m | [33m84743:8778162368[0m | [36magentbay.context_manager[0m:[36minfo[0m:[36m98[0m | [1mâœ… API Response received[0m
[32m2025-10-21 11:23:10.927[0m | [1m[34mAgentBay[0m[1m[0m | [1mINFO[0m | [33m84743:8778162368[0m | [36magentbay.agentbay[0m:[36m_wait_for_context_synchronization[0m:[36m265[0m | [1mðŸ“ Context SdkCtx-04b2mq8td7fdvhtl2 status: Success, path: /temp/file-transfer[0m
[32m2025-10-21 11:23:10.928[0m | [1m[34mAgentBay[0m[1m[0m | [1mINFO[0m | [33m84743:8778162368[0m | [36magentbay.agentbay[0m:[36m_wait_for_context_synchronization[0m:[36m283[0m | [1mâœ… Completed: Context synchronization[0m
DEBUG    urllib3.connectionpool:connectionpool.py:544 https://wuyingai.cn-shanghai.aliyuncs.com:443 "POST /?Action=CallMcpTool&Format=json&Version=2025-05-06&Timestamp=2025-10-21T03%3A23%3A10Z&SignatureNonce=31e890a59666e66d84fb15e56f39f2cd HTTP/1.1" 200 222
[32m2025-10-21 11:23:11.325[0m | [1m[34mAgentBay[0m[1m[0m | [1mINFO[0m | [33m84743:8778162368[0m | [36magentbay.api.base_service[0m:[36m_call_mcp_tool[0m:[36m195[0m | [1mâœ… API Response received[0m
DEBUG    urllib3.connectionpool:connectionpool.py:544 https://wuyingai.cn-shanghai.aliyuncs.com:443 "POST /?Action=ReleaseMcpSession&Format=json&Version=2025-05-06&Timestamp=2025-10-21T03%3A23%3A11Z&SignatureNonce=fa34ef6c57a6e2d22788981c077faad7 HTTP/1.1" 200 100
[32m2025-10-21 11:23:11.626[0m | [1m[34mAgentBay[0m[1m[0m | [1mINFO[0m | [33m84743:8778162368[0m | [36magentbay.session[0m:[36mdelete[0m:[36m212[0m | [1mâœ… API Response received[0m
ðŸš€ Crew: crew
â””â”€â”€ ðŸ“‹ Task: run_code_task (ID: 61d18051-43cc-4f41-adaf-d58061d2a59b)
    Status: Executing Task...
    â””â”€â”€ ðŸ”§ Used agentbay_run_code (1)â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ðŸ”§ Agent Tool Execution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                                                                                                                                                                                                    â”‚
â”‚  Agent: Cloud Code Execution Agent                                                                                                                                                                                 â”‚
â”‚                                                                                                                                                                                                                    â”‚
â”‚  Thought: Thought: I need to execute the provided Python code snippet in the cloud AgentBay session.                                                                                                               â”‚
â”‚                                                                                                                                                                                                                    â”‚
â”‚  Using Tool: agentbay_run_code                                                                                                                                                                                     â”‚
â”‚                                                                                                                                                                                                                    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Tool Input â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                                                                                                                                                                                                    â”‚
â”‚  {                                                                                                                                                                                                                 â”‚
â”‚    "code": "print('hello from agentbay')",                                                                                                                                                                         â”‚
â”‚    "language": "python",                                                                                                                                                                                           â”‚
â”‚    "timeout_s": 60                                                                                                                                                                                                 â”‚
â”‚  }                                                                                                                                                                                                                 â”‚
â”‚                                                                                                                                                                                                                    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Tool Output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                                                                                                                                                                                                    â”‚
â”‚  hello from agentbay                                                                                                                                                                                               â”‚
â”‚                                                                                                                                                                                                                    â”‚
â”‚                                                                                                                                                                                                                    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

[92m11:23:11 - LiteLLM:DEBUG[0m: utils.py:349 - 

DEBUG    LiteLLM:utils.py:349 

[92m11:23:11 - LiteLLM:DEBUG[0m: utils.py:349 - [92mRequest to litellm:[0m
DEBUG    LiteLLM:utils.py:349 [92mRequest to litellm:[0m
[92m11:23:11 - LiteLLM:DEBUG[0m: utils.py:349 - [92mlitellm.completion(model='openai/qwen-plus', messages=[{'role': 'system', 'content': 'You are Cloud Code Execution Agent. Experienced with remote sessions and secure sandboxes, capable of reliably executing scripts and reporting output.\nYour personal goal is: Execute given code in a secure, controlled cloud environment and return results\nYou ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n\nTool Name: agentbay_run_code\nTool Arguments: {\'code\': {\'description\': \'Code content to execute\', \'type\': \'str\'}, \'language\': {\'description\': \'Programming language, either python or javascript\', \'type\': \'str\'}, \'timeout_s\': {\'description\': \'Timeout in seconds, max 60s\', \'type\': \'int\'}, \'labels\': {\'description\': \'Optional: session labels\', \'type\': \'Union[dict[str, str], NoneType]\'}}\nTool Description: Execute code (python/javascript) in AgentBay cloud session and return stdout text. Input should include: code (the code to execute), language (python or javascript, default: python), timeout_s (execution timeout in seconds, default: 60), and optional labels (dict).\n\nIMPORTANT: Use the following format in your response:\n\n```\nThought: you should always think about what to do\nAction: the action to take, only one name of [agentbay_run_code], just the name, exactly as it\'s written.\nAction Input: the input to the action, just a simple JSON object, enclosed in curly braces, using " to wrap keys and values.\nObservation: the result of the action\n```\n\nOnce all necessary information is gathered, return the following format:\n\n```\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n```'}, {'role': 'user', 'content': '\nCurrent Task: """ Run a python code snippet in cloud AgentBay session and return the output. You should only return the execution result text without additional explanations.\nCode: ---------- print(\'hello from agentbay\') """\n\n\nThis is the expected criteria for your final answer: """ Plain text execution result (stdout), or error message if an exception occurred. """\n\nyou MUST return the actual complete content as the final answer, not a summary.\n\nBegin! This is VERY important to you, use the tools available and give your best Final Answer, your job depends on it!\n\nThought:'}, {'role': 'assistant', 'content': 'Thought: I need to execute the provided Python code snippet in the cloud AgentBay session.\nAction: agentbay_run_code\nAction Input: {"code": "print(\'hello from agentbay\')", "language": "python", "timeout_s": 60}\nObservation: hello from agentbay'}], stop=['\nObservation:'], api_base='https://dashscope.aliyuncs.com/compatible-mode/v1', base_url='https://dashscope.aliyuncs.com/compatible-mode/v1', api_key='sk-5767c27c7c41401ab7556f8c06e7ea50', stream=False)[0m
DEBUG    LiteLLM:utils.py:349 [92mlitellm.completion(model='openai/qwen-plus', messages=[{'role': 'system', 'content': 'You are Cloud Code Execution Agent. Experienced with remote sessions and secure sandboxes, capable of reliably executing scripts and reporting output.\nYour personal goal is: Execute given code in a secure, controlled cloud environment and return results\nYou ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n\nTool Name: agentbay_run_code\nTool Arguments: {\'code\': {\'description\': \'Code content to execute\', \'type\': \'str\'}, \'language\': {\'description\': \'Programming language, either python or javascript\', \'type\': \'str\'}, \'timeout_s\': {\'description\': \'Timeout in seconds, max 60s\', \'type\': \'int\'}, \'labels\': {\'description\': \'Optional: session labels\', \'type\': \'Union[dict[str, str], NoneType]\'}}\nTool Description: Execute code (python/javascript) in AgentBay cloud session and return stdout text. Input should include: code (the code to execute), language (python or javascript, default: python), timeout_s (execution timeout in seconds, default: 60), and optional labels (dict).\n\nIMPORTANT: Use the following format in your response:\n\n```\nThought: you should always think about what to do\nAction: the action to take, only one name of [agentbay_run_code], just the name, exactly as it\'s written.\nAction Input: the input to the action, just a simple JSON object, enclosed in curly braces, using " to wrap keys and values.\nObservation: the result of the action\n```\n\nOnce all necessary information is gathered, return the following format:\n\n```\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n```'}, {'role': 'user', 'content': '\nCurrent Task: """ Run a python code snippet in cloud AgentBay session and return the output. You should only return the execution result text without additional explanations.\nCode: ---------- print(\'hello from agentbay\') """\n\n\nThis is the expected criteria for your final answer: """ Plain text execution result (stdout), or error message if an exception occurred. """\n\nyou MUST return the actual complete content as the final answer, not a summary.\n\nBegin! This is VERY important to you, use the tools available and give your best Final Answer, your job depends on it!\n\nThought:'}, {'role': 'assistant', 'content': 'Thought: I need to execute the provided Python code snippet in the cloud AgentBay session.\nAction: agentbay_run_code\nAction Input: {"code": "print(\'hello from agentbay\')", "language": "python", "timeout_s": 60}\nObservation: hello from agentbay'}], stop=['\nObservation:'], api_base='https://dashscope.aliyuncs.com/compatible-mode/v1', base_url='https://dashscope.aliyuncs.com/compatible-mode/v1', api_key='sk-5767c27c7c41401ab7556f8c06e7ea50', stream=False)[0m
[92m11:23:11 - LiteLLM:DEBUG[0m: utils.py:349 - 

DEBUG    LiteLLM:utils.py:349 

[92m11:23:11 - LiteLLM:DEBUG[0m: utils.py:349 - Initialized litellm callbacks, Async Success Callbacks: [<crewai.utilities.token_counter_callback.TokenCalcHandler object at 0x11c14e8b0>]
DEBUG    LiteLLM:utils.py:349 Initialized litellm callbacks, Async Success Callbacks: [<crewai.utilities.token_counter_callback.TokenCalcHandler object at 0x11c14e8b0>]
[92m11:23:11 - LiteLLM:DEBUG[0m: litellm_logging.py:475 - self.optional_params: {}
DEBUG    LiteLLM:litellm_logging.py:475 self.optional_params: {}
[92m11:23:11 - LiteLLM:DEBUG[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
DEBUG    LiteLLM:utils.py:349 SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
[92m11:23:11 - LiteLLM:INFO[0m: utils.py:3258 - 
LiteLLM completion() model= qwen-plus; provider = openai
INFO     LiteLLM:utils.py:3258 
LiteLLM completion() model= qwen-plus; provider = openai
[92m11:23:11 - LiteLLM:DEBUG[0m: utils.py:3261 - 
LiteLLM: Params passed to completion() {'model': 'qwen-plus', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': ['\nObservation:'], 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'openai', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'You are Cloud Code Execution Agent. Experienced with remote sessions and secure sandboxes, capable of reliably executing scripts and reporting output.\nYour personal goal is: Execute given code in a secure, controlled cloud environment and return results\nYou ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n\nTool Name: agentbay_run_code\nTool Arguments: {\'code\': {\'description\': \'Code content to execute\', \'type\': \'str\'}, \'language\': {\'description\': \'Programming language, either python or javascript\', \'type\': \'str\'}, \'timeout_s\': {\'description\': \'Timeout in seconds, max 60s\', \'type\': \'int\'}, \'labels\': {\'description\': \'Optional: session labels\', \'type\': \'Union[dict[str, str], NoneType]\'}}\nTool Description: Execute code (python/javascript) in AgentBay cloud session and return stdout text. Input should include: code (the code to execute), language (python or javascript, default: python), timeout_s (execution timeout in seconds, default: 60), and optional labels (dict).\n\nIMPORTANT: Use the following format in your response:\n\n```\nThought: you should always think about what to do\nAction: the action to take, only one name of [agentbay_run_code], just the name, exactly as it\'s written.\nAction Input: the input to the action, just a simple JSON object, enclosed in curly braces, using " to wrap keys and values.\nObservation: the result of the action\n```\n\nOnce all necessary information is gathered, return the following format:\n\n```\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n```'}, {'role': 'user', 'content': '\nCurrent Task: """ Run a python code snippet in cloud AgentBay session and return the output. You should only return the execution result text without additional explanations.\nCode: ---------- print(\'hello from agentbay\') """\n\n\nThis is the expected criteria for your final answer: """ Plain text execution result (stdout), or error message if an exception occurred. """\n\nyou MUST return the actual complete content as the final answer, not a summary.\n\nBegin! This is VERY important to you, use the tools available and give your best Final Answer, your job depends on it!\n\nThought:'}, {'role': 'assistant', 'content': 'Thought: I need to execute the provided Python code snippet in the cloud AgentBay session.\nAction: agentbay_run_code\nAction Input: {"code": "print(\'hello from agentbay\')", "language": "python", "timeout_s": 60}\nObservation: hello from agentbay'}], 'thinking': None, 'web_search_options': None}
DEBUG    LiteLLM:utils.py:3261 
LiteLLM: Params passed to completion() {'model': 'qwen-plus', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': ['\nObservation:'], 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'openai', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'You are Cloud Code Execution Agent. Experienced with remote sessions and secure sandboxes, capable of reliably executing scripts and reporting output.\nYour personal goal is: Execute given code in a secure, controlled cloud environment and return results\nYou ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n\nTool Name: agentbay_run_code\nTool Arguments: {\'code\': {\'description\': \'Code content to execute\', \'type\': \'str\'}, \'language\': {\'description\': \'Programming language, either python or javascript\', \'type\': \'str\'}, \'timeout_s\': {\'description\': \'Timeout in seconds, max 60s\', \'type\': \'int\'}, \'labels\': {\'description\': \'Optional: session labels\', \'type\': \'Union[dict[str, str], NoneType]\'}}\nTool Description: Execute code (python/javascript) in AgentBay cloud session and return stdout text. Input should include: code (the code to execute), language (python or javascript, default: python), timeout_s (execution timeout in seconds, default: 60), and optional labels (dict).\n\nIMPORTANT: Use the following format in your response:\n\n```\nThought: you should always think about what to do\nAction: the action to take, only one name of [agentbay_run_code], just the name, exactly as it\'s written.\nAction Input: the input to the action, just a simple JSON object, enclosed in curly braces, using " to wrap keys and values.\nObservation: the result of the action\n```\n\nOnce all necessary information is gathered, return the following format:\n\n```\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n```'}, {'role': 'user', 'content': '\nCurrent Task: """ Run a python code snippet in cloud AgentBay session and return the output. You should only return the execution result text without additional explanations.\nCode: ---------- print(\'hello from agentbay\') """\n\n\nThis is the expected criteria for your final answer: """ Plain text execution result (stdout), or error message if an exception occurred. """\n\nyou MUST return the actual complete content as the final answer, not a summary.\n\nBegin! This is VERY important to you, use the tools available and give your best Final Answer, your job depends on it!\n\nThought:'}, {'role': 'assistant', 'content': 'Thought: I need to execute the provided Python code snippet in the cloud AgentBay session.\nAction: agentbay_run_code\nAction Input: {"code": "print(\'hello from agentbay\')", "language": "python", "timeout_s": 60}\nObservation: hello from agentbay'}], 'thinking': None, 'web_search_options': None}
[92m11:23:11 - LiteLLM:DEBUG[0m: utils.py:3264 - 
LiteLLM: Non-Default params passed to completion() {'stream': False, 'stop': ['\nObservation:']}
DEBUG    LiteLLM:utils.py:3264 
LiteLLM: Non-Default params passed to completion() {'stream': False, 'stop': ['\nObservation:']}
[92m11:23:11 - LiteLLM:DEBUG[0m: utils.py:349 - Final returned optional params: {'stream': False, 'stop': ['\nObservation:'], 'extra_body': {}}
DEBUG    LiteLLM:utils.py:349 Final returned optional params: {'stream': False, 'stop': ['\nObservation:'], 'extra_body': {}}
[92m11:23:11 - LiteLLM:DEBUG[0m: litellm_logging.py:475 - self.optional_params: {'stream': False, 'stop': ['\nObservation:'], 'extra_body': {}}
DEBUG    LiteLLM:litellm_logging.py:475 self.optional_params: {'stream': False, 'stop': ['\nObservation:'], 'extra_body': {}}
[92m11:23:11 - LiteLLM:DEBUG[0m: utils.py:4620 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
DEBUG    LiteLLM:utils.py:4620 checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
[92m11:23:11 - LiteLLM:DEBUG[0m: utils.py:4835 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
DEBUG    LiteLLM:utils.py:4835 Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m11:23:11 - LiteLLM:DEBUG[0m: main.py:858 - Error getting model info: This model isn't mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
DEBUG    LiteLLM:main.py:858 Error getting model info: This model isn't mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
[92m11:23:11 - LiteLLM:DEBUG[0m: litellm_logging.py:923 - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://dashscope.aliyuncs.com/compatible-mode/v1/ \
-d '{'model': 'qwen-plus', 'messages': [{'role': 'system', 'content': 'You are Cloud Code Execution Agent. Experienced with remote sessions and secure sandboxes, capable of reliably executing scripts and reporting output.\nYour personal goal is: Execute given code in a secure, controlled cloud environment and return results\nYou ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n\nTool Name: agentbay_run_code\nTool Arguments: {\'code\': {\'description\': \'Code content to execute\', \'type\': \'str\'}, \'language\': {\'description\': \'Programming language, either python or javascript\', \'type\': \'str\'}, \'timeout_s\': {\'description\': \'Timeout in seconds, max 60s\', \'type\': \'int\'}, \'labels\': {\'description\': \'Optional: session labels\', \'type\': \'Union[dict[str, str], NoneType]\'}}\nTool Description: Execute code (python/javascript) in AgentBay cloud session and return stdout text. Input should include: code (the code to execute), language (python or javascript, default: python), timeout_s (execution timeout in seconds, default: 60), and optional labels (dict).\n\nIMPORTANT: Use the following format in your response:\n\n```\nThought: you should always think about what to do\nAction: the action to take, only one name of [agentbay_run_code], just the name, exactly as it\'s written.\nAction Input: the input to the action, just a simple JSON object, enclosed in curly braces, using " to wrap keys and values.\nObservation: the result of the action\n```\n\nOnce all necessary information is gathered, return the following format:\n\n```\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n```'}, {'role': 'user', 'content': '\nCurrent Task: """ Run a python code snippet in cloud AgentBay session and return the output. You should only return the execution result text without additional explanations.\nCode: ---------- print(\'hello from agentbay\') """\n\n\nThis is the expected criteria for your final answer: """ Plain text execution result (stdout), or error message if an exception occurred. """\n\nyou MUST return the actual complete content as the final answer, not a summary.\n\nBegin! This is VERY important to you, use the tools available and give your best Final Answer, your job depends on it!\n\nThought:'}, {'role': 'assistant', 'content': 'Thought: I need to execute the provided Python code snippet in the cloud AgentBay session.\nAction: agentbay_run_code\nAction Input: {"code": "print(\'hello from agentbay\')", "language": "python", "timeout_s": 60}\nObservation: hello from agentbay'}], 'stop': ['\nObservation:'], 'extra_body': {}}'
[0m

DEBUG    LiteLLM:litellm_logging.py:923 [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://dashscope.aliyuncs.com/compatible-mode/v1/ \
-d '{'model': 'qwen-plus', 'messages': [{'role': 'system', 'content': 'You are Cloud Code Execution Agent. Experienced with remote sessions and secure sandboxes, capable of reliably executing scripts and reporting output.\nYour personal goal is: Execute given code in a secure, controlled cloud environment and return results\nYou ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n\nTool Name: agentbay_run_code\nTool Arguments: {\'code\': {\'description\': \'Code content to execute\', \'type\': \'str\'}, \'language\': {\'description\': \'Programming language, either python or javascript\', \'type\': \'str\'}, \'timeout_s\': {\'description\': \'Timeout in seconds, max 60s\', \'type\': \'int\'}, \'labels\': {\'description\': \'Optional: session labels\', \'type\': \'Union[dict[str, str], NoneType]\'}}\nTool Description: Execute code (python/javascript) in AgentBay cloud session and return stdout text. Input should include: code (the code to execute), language (python or javascript, default: python), timeout_s (execution timeout in seconds, default: 60), and optional labels (dict).\n\nIMPORTANT: Use the following format in your response:\n\n```\nThought: you should always think about what to do\nAction: the action to take, only one name of [agentbay_run_code], just the name, exactly as it\'s written.\nAction Input: the input to the action, just a simple JSON object, enclosed in curly braces, using " to wrap keys and values.\nObservation: the result of the action\n```\n\nOnce all necessary information is gathered, return the following format:\n\n```\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n```'}, {'role': 'user', 'content': '\nCurrent Task: """ Run a python code snippet in cloud AgentBay session and return the output. You should only return the execution result text without additional explanations.\nCode: ---------- print(\'hello from agentbay\') """\n\n\nThis is the expected criteria for your final answer: """ Plain text execution result (stdout), or error message if an exception occurred. """\n\nyou MUST return the actual complete content as the final answer, not a summary.\n\nBegin! This is VERY important to you, use the tools available and give your best Final Answer, your job depends on it!\n\nThought:'}, {'role': 'assistant', 'content': 'Thought: I need to execute the provided Python code snippet in the cloud AgentBay session.\nAction: agentbay_run_code\nAction Input: {"code": "print(\'hello from agentbay\')", "language": "python", "timeout_s": 60}\nObservation: hello from agentbay'}], 'stop': ['\nObservation:'], 'extra_body': {}}'
[0m

DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'timeout': 600.0, 'files': None, 'idempotency_key': 'stainless-python-retry-5ed4befa-13e2-4599-9817-c4823ba6b4d5', 'json_data': {'messages': [{'role': 'system', 'content': 'You are Cloud Code Execution Agent. Experienced with remote sessions and secure sandboxes, capable of reliably executing scripts and reporting output.\nYour personal goal is: Execute given code in a secure, controlled cloud environment and return results\nYou ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n\nTool Name: agentbay_run_code\nTool Arguments: {\'code\': {\'description\': \'Code content to execute\', \'type\': \'str\'}, \'language\': {\'description\': \'Programming language, either python or javascript\', \'type\': \'str\'}, \'timeout_s\': {\'description\': \'Timeout in seconds, max 60s\', \'type\': \'int\'}, \'labels\': {\'description\': \'Optional: session labels\', \'type\': \'Union[dict[str, str], NoneType]\'}}\nTool Description: Execute code (python/javascript) in AgentBay cloud session and return stdout text. Input should include: code (the code to execute), language (python or javascript, default: python), timeout_s (execution timeout in seconds, default: 60), and optional labels (dict).\n\nIMPORTANT: Use the following format in your response:\n\n```\nThought: you should always think about what to do\nAction: the action to take, only one name of [agentbay_run_code], just the name, exactly as it\'s written.\nAction Input: the input to the action, just a simple JSON object, enclosed in curly braces, using " to wrap keys and values.\nObservation: the result of the action\n```\n\nOnce all necessary information is gathered, return the following format:\n\n```\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n```'}, {'role': 'user', 'content': '\nCurrent Task: """ Run a python code snippet in cloud AgentBay session and return the output. You should only return the execution result text without additional explanations.\nCode: ---------- print(\'hello from agentbay\') """\n\n\nThis is the expected criteria for your final answer: """ Plain text execution result (stdout), or error message if an exception occurred. """\n\nyou MUST return the actual complete content as the final answer, not a summary.\n\nBegin! This is VERY important to you, use the tools available and give your best Final Answer, your job depends on it!\n\nThought:'}, {'role': 'assistant', 'content': 'Thought: I need to execute the provided Python code snippet in the cloud AgentBay session.\nAction: agentbay_run_code\nAction Input: {"code": "print(\'hello from agentbay\')", "language": "python", "timeout_s": 60}\nObservation: hello from agentbay'}], 'model': 'qwen-plus', 'stop': ['\nObservation:']}, 'extra_json': {}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='dashscope.aliyuncs.com' port=443 local_address=None timeout=600.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11c2f6fd0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x11c133920> server_hostname='dashscope.aliyuncs.com' timeout=600.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11c9368d0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'62feb87a-3ca5-42e4-8fcf-d0d16adca445'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'845'), (b'req-arrive-time', b'1761016991834'), (b'resp-start-time', b'1761016992680'), (b'x-envoy-upstream-service-time', b'844'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 21 Oct 2025 03:23:12 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '62feb87a-3ca5-42e4-8fcf-d0d16adca445', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '845', 'req-arrive-time': '1761016991834', 'resp-start-time': '1761016992680', 'x-envoy-upstream-service-time': '844', 'content-encoding': 'gzip', 'date': 'Tue, 21 Oct 2025 03:23:12 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 62feb87a-3ca5-42e4-8fcf-d0d16adca445
[92m11:23:12 - LiteLLM:DEBUG[0m: utils.py:349 - RAW RESPONSE:
{"id": "chatcmpl-62feb87a-3ca5-42e4-8fcf-d0d16adca445", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "Thought: I now know the final answer\nFinal Answer: hello from agentbay", "refusal": null, "role": "assistant", "annotations": null, "audio": null, "function_call": null, "tool_calls": null}}], "created": 1761016993, "model": "qwen-plus", "object": "chat.completion", "service_tier": null, "system_fingerprint": null, "usage": {"completion_tokens": 16, "prompt_tokens": 539, "total_tokens": 555, "completion_tokens_details": null, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0}}}


DEBUG    LiteLLM:utils.py:349 RAW RESPONSE:
{"id": "chatcmpl-62feb87a-3ca5-42e4-8fcf-d0d16adca445", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "Thought: I now know the final answer\nFinal Answer: hello from agentbay", "refusal": null, "role": "assistant", "annotations": null, "audio": null, "function_call": null, "tool_calls": null}}], "created": 1761016993, "model": "qwen-plus", "object": "chat.completion", "service_tier": null, "system_fingerprint": null, "usage": {"completion_tokens": 16, "prompt_tokens": 539, "total_tokens": 555, "completion_tokens_details": null, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0}}}


[92m11:23:12 - LiteLLM:INFO[0m: utils.py:1260 - Wrapper: Completed Call, calling success_handler
INFO     LiteLLM:utils.py:1260 Wrapper: Completed Call, calling success_handler
[92m11:23:12 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: openai/qwen-plus
DEBUG    LiteLLM:cost_calculator.py:675 selected model name for cost calculation: openai/qwen-plus
[92m11:23:12 - LiteLLM:DEBUG[0m: litellm_logging.py:1595 - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG    LiteLLM:litellm_logging.py:1595 Logging Details LiteLLM-Success Call: Cache_hit=None
[92m11:23:12 - LiteLLM:DEBUG[0m: utils.py:4620 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
DEBUG    LiteLLM:utils.py:4620 checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
[92m11:23:12 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: openai/qwen-plus
DEBUG    LiteLLM:cost_calculator.py:675 selected model name for cost calculation: openai/qwen-plus
[92m11:23:12 - LiteLLM:DEBUG[0m: utils.py:4835 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
DEBUG    LiteLLM:utils.py:4835 Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m11:23:12 - LiteLLM:DEBUG[0m: utils.py:4620 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
DEBUG    LiteLLM:utils.py:4620 checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
[92m11:23:12 - LiteLLM:DEBUG[0m: cost_calculator.py:947 - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/qwen-plus - This model isn't mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
DEBUG    LiteLLM:cost_calculator.py:947 litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/qwen-plus - This model isn't mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
[92m11:23:12 - LiteLLM:DEBUG[0m: utils.py:4835 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
DEBUG    LiteLLM:utils.py:4835 Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m11:23:12 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: openai/qwen-plus
DEBUG    LiteLLM:cost_calculator.py:675 selected model name for cost calculation: openai/qwen-plus
[92m11:23:12 - LiteLLM:DEBUG[0m: cost_calculator.py:947 - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/qwen-plus - This model isn't mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
DEBUG    LiteLLM:cost_calculator.py:947 litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/qwen-plus - This model isn't mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
[92m11:23:12 - LiteLLM:DEBUG[0m: utils.py:4620 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
DEBUG    LiteLLM:utils.py:4620 checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
[92m11:23:12 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: qwen-plus
DEBUG    LiteLLM:cost_calculator.py:675 selected model name for cost calculation: qwen-plus
[92m11:23:12 - LiteLLM:DEBUG[0m: utils.py:4835 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
DEBUG    LiteLLM:utils.py:4835 Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m11:23:12 - LiteLLM:DEBUG[0m: utils.py:4620 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
DEBUG    LiteLLM:utils.py:4620 checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
[92m11:23:12 - LiteLLM:DEBUG[0m: cost_calculator.py:947 - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/qwen-plus - This model isn't mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
DEBUG    LiteLLM:cost_calculator.py:947 litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/qwen-plus - This model isn't mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
[92m11:23:12 - LiteLLM:DEBUG[0m: utils.py:4835 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
DEBUG    LiteLLM:utils.py:4835 Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m11:23:12 - LiteLLM:DEBUG[0m: litellm_logging.py:1239 - response_cost_failure_debug_information: {'error_str': "This model isn't mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.", 'traceback_str': 'Traceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4709, in _get_model_info_helper\n    raise ValueError(\n        "This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json"\n    )\nValueError: This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 1221, in _response_cost_calculator\n    response_cost = litellm.response_cost_calculator(\n        **response_cost_calculator_kwargs\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1062, in response_cost_calculator\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1046, in response_cost_calculator\n    response_cost = completion_cost(\n        completion_response=response_object,\n    ...<10 lines>...\n        litellm_logging_obj=litellm_logging_obj,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 960, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 953, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 915, in completion_cost\n    ) = cost_per_token(\n        ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<14 lines>...\n        rerank_billed_units=rerank_billed_units,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 335, in cost_per_token\n    return openai_cost_per_token(model=model, usage=usage_block)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/llms/openai/cost_calculation.py", line 33, in cost_per_token\n    return generic_cost_per_token(\n        model=model, usage=usage, custom_llm_provider="openai"\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py", line 207, in generic_cost_per_token\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4920, in get_model_info\n    _model_info = _get_model_info_helper(\n        model=model,\n        custom_llm_provider=custom_llm_provider,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4838, in _get_model_info_helper\n    raise Exception(\n    ...<3 lines>...\n    )\nException: This model isn\'t mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n', 'model': 'openai/qwen-plus', 'cache_hit': False, 'custom_llm_provider': 'openai', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}
DEBUG    LiteLLM:litellm_logging.py:1239 response_cost_failure_debug_information: {'error_str': "This model isn't mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.", 'traceback_str': 'Traceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4709, in _get_model_info_helper\n    raise ValueError(\n        "This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json"\n    )\nValueError: This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 1221, in _response_cost_calculator\n    response_cost = litellm.response_cost_calculator(\n        **response_cost_calculator_kwargs\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1062, in response_cost_calculator\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1046, in response_cost_calculator\n    response_cost = completion_cost(\n        completion_response=response_object,\n    ...<10 lines>...\n        litellm_logging_obj=litellm_logging_obj,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 960, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 953, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 915, in completion_cost\n    ) = cost_per_token(\n        ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<14 lines>...\n        rerank_billed_units=rerank_billed_units,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 335, in cost_per_token\n    return openai_cost_per_token(model=model, usage=usage_block)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/llms/openai/cost_calculation.py", line 33, in cost_per_token\n    return generic_cost_per_token(\n        model=model, usage=usage, custom_llm_provider="openai"\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py", line 207, in generic_cost_per_token\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4920, in get_model_info\n    _model_info = _get_model_info_helper(\n        model=model,\n        custom_llm_provider=custom_llm_provider,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4838, in _get_model_info_helper\n    raise Exception(\n    ...<3 lines>...\n    )\nException: This model isn\'t mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n', 'model': 'openai/qwen-plus', 'cache_hit': False, 'custom_llm_provider': 'openai', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}
[92m11:23:12 - LiteLLM:DEBUG[0m: cost_calculator.py:947 - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=qwen-plus - This model isn't mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
ðŸš€ Crew: crew
â””â”€â”€ ðŸ“‹ Task: run_code_task (ID: 61d18051-43cc-4f41-adaf-d58061d2a59b)
    Status: Executing Task...
    â””â”€â”€ ðŸ”§ Used agentbay_run_code (1)DEBUG    LiteLLM:cost_calculator.py:947 litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=qwen-plus - This model isn't mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
[92m11:23:12 - LiteLLM:DEBUG[0m: litellm_logging.py:1239 - response_cost_failure_debug_information: {'error_str': "This model isn't mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.", 'traceback_str': 'Traceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4709, in _get_model_info_helper\n    raise ValueError(\n        "This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json"\n    )\nValueError: This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 1221, in _response_cost_calculator\n    response_cost = litellm.response_cost_calculator(\n        **response_cost_calculator_kwargs\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1062, in response_cost_calculator\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1046, in response_cost_calculator\n    response_cost = completion_cost(\n        completion_response=response_object,\n    ...<10 lines>...\n        litellm_logging_obj=litellm_logging_obj,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 960, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 953, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 915, in completion_cost\n    ) = cost_per_token(\n        ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<14 lines>...\n        rerank_billed_units=rerank_billed_units,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 335, in cost_per_token\n    return openai_cost_per_token(model=model, usage=usage_block)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/llms/openai/cost_calculation.py", line 33, in cost_per_token\n    return generic_cost_per_token(\n        model=model, usage=usage, custom_llm_provider="openai"\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py", line 207, in generic_cost_per_token\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4920, in get_model_info\n    _model_info = _get_model_info_helper(\n        model=model,\n        custom_llm_provider=custom_llm_provider,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4838, in _get_model_info_helper\n    raise Exception(\n    ...<3 lines>...\n    )\nException: This model isn\'t mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n', 'model': 'qwen-plus', 'cache_hit': None, 'custom_llm_provider': 'openai', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ âœ… Agent Final Answer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                                                                                                                                                                                                    â”‚
â”‚  Agent: Cloud Code Execution Agent                                                                                                                                                                                 â”‚
â”‚                                                                                                                                                                                                                    â”‚
â”‚  Final Answer:                                                                                                                                                                                                     â”‚
â”‚  hello from agentbay                                                                                                                                                                                               â”‚
â”‚                                                                                                                                                                                                                    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
DEBUG    LiteLLM:litellm_logging.py:1239 response_cost_failure_debug_information: {'error_str': "This model isn't mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.", 'traceback_str': 'Traceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4709, in _get_model_info_helper\n    raise ValueError(\n        "This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json"\n    )\nValueError: This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 1221, in _response_cost_calculator\n    response_cost = litellm.response_cost_calculator(\n        **response_cost_calculator_kwargs\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1062, in response_cost_calculator\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1046, in response_cost_calculator\n    response_cost = completion_cost(\n        completion_response=response_object,\n    ...<10 lines>...\n        litellm_logging_obj=litellm_logging_obj,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 960, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 953, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 915, in completion_cost\n    ) = cost_per_token(\n        ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<14 lines>...\n        rerank_billed_units=rerank_billed_units,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 335, in cost_per_token\n    return openai_cost_per_token(model=model, usage=usage_block)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/llms/openai/cost_calculation.py", line 33, in cost_per_token\n    return generic_cost_per_token(\n        model=model, usage=usage, custom_llm_provider="openai"\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py", line 207, in generic_cost_per_token\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4920, in get_model_info\n    _model_info = _get_model_info_helper(\n        model=model,\n        custom_llm_provider=custom_llm_provider,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4838, in _get_model_info_helper\n    raise Exception(\n    ...<3 lines>...\n    )\nException: This model isn\'t mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n', 'model': 'qwen-plus', 'cache_hit': None, 'custom_llm_provider': 'openai', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}
[92m11:23:12 - LiteLLM:DEBUG[0m: utils.py:4620 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}

DEBUG    LiteLLM:utils.py:4620 checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
[92m11:23:12 - LiteLLM:DEBUG[0m: utils.py:4835 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
ðŸš€ Crew: crew
â””â”€â”€ ðŸ“‹ Task: run_code_task (ID: 61d18051-43cc-4f41-adaf-d58061d2a59b)
    Assigned to: Cloud Code Execution Agent
    Status: âœ… Completed
    â””â”€â”€ ðŸ”§ Used agentbay_run_code (1)DEBUG    LiteLLM:utils.py:4835 Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m11:23:12 - LiteLLM:DEBUG[0m: litellm_logging.py:3956 - Model=qwen-plus is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Task Completion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                                                                                                                                                                                                    â”‚
â”‚  Task Completed                                                                                                                                                                                                    â”‚
â”‚  Name: run_code_task                                                                                                                                                                                               â”‚
â”‚  Agent: Cloud Code Execution Agent                                                                                                                                                                                 â”‚
â”‚  Tool Args:                                                                                                                                                                                                        â”‚
â”‚                                                                                                                                                                                                                    â”‚
â”‚                                                                                                                                                                                                                    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
DEBUG    LiteLLM:litellm_logging.py:3956 Model=qwen-plus is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload
[92m11:23:12 - LiteLLM:DEBUG[0m: litellm_logging.py:1624 - Logging Details LiteLLM-Success Call streaming complete

DEBUG    LiteLLM:litellm_logging.py:1624 Logging Details LiteLLM-Success Call streaming complete
[92m11:23:12 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: openai/qwen-plus
DEBUG    LiteLLM:cost_calculator.py:675 selected model name for cost calculation: openai/qwen-plus
[92m11:23:12 - LiteLLM:DEBUG[0m: utils.py:4620 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
DEBUG    LiteLLM:utils.py:4620 checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
[92m11:23:12 - LiteLLM:DEBUG[0m: utils.py:4835 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
DEBUG    LiteLLM:utils.py:4835 Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m11:23:12 - LiteLLM:DEBUG[0m: cost_calculator.py:947 - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/qwen-plus - This model isn't mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
DEBUG    LiteLLM:cost_calculator.py:947 litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/qwen-plus - This model isn't mapped yet. model=openai/qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
[92m11:23:12 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: qwen-plus
DEBUG    LiteLLM:cost_calculator.py:675 selected model name for cost calculation: qwen-plus
[92m11:23:12 - LiteLLM:DEBUG[0m: utils.py:4620 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
DEBUG    LiteLLM:utils.py:4620 checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
[92m11:23:12 - LiteLLM:DEBUG[0m: utils.py:4835 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
DEBUG    LiteLLM:utils.py:4835 Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m11:23:12 - LiteLLM:DEBUG[0m: cost_calculator.py:947 - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=qwen-plus - This model isn't mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
DEBUG    LiteLLM:cost_calculator.py:947 litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=qwen-plus - This model isn't mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
[92m11:23:12 - LiteLLM:DEBUG[0m: litellm_logging.py:1239 - response_cost_failure_debug_information: {'error_str': "This model isn't mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.", 'traceback_str': 'Traceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4709, in _get_model_info_helper\n    raise ValueError(\n        "This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json"\n    )\nValueError: This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 1221, in _response_cost_calculator\n    response_cost = litellm.response_cost_calculator(\n        **response_cost_calculator_kwargs\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1062, in response_cost_calculator\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1046, in response_cost_calculator\n    response_cost = completion_cost(\n        completion_response=response_object,\n    ...<10 lines>...\n        litellm_logging_obj=litellm_logging_obj,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 960, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 953, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 915, in completion_cost\n    ) = cost_per_token(\n        ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<14 lines>...\n        rerank_billed_units=rerank_billed_units,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 335, in cost_per_token\n    return openai_cost_per_token(model=model, usage=usage_block)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/llms/openai/cost_calculation.py", line 33, in cost_per_token\n    return generic_cost_per_token(\n        model=model, usage=usage, custom_llm_provider="openai"\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py", line 207, in generic_cost_per_token\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4920, in get_model_info\n    _model_info = _get_model_info_helper(\n        model=model,\n        custom_llm_provider=custom_llm_provider,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4838, in _get_model_info_helper\n    raise Exception(\n    ...<3 lines>...\n    )\nException: This model isn\'t mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n', 'model': 'qwen-plus', 'cache_hit': None, 'custom_llm_provider': 'openai', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}
DEBUG    LiteLLM:litellm_logging.py:1239 response_cost_failure_debug_information: {'error_str': "This model isn't mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.", 'traceback_str': 'Traceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4709, in _get_model_info_helper\n    raise ValueError(\n        "This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json"\n    )\nValueError: This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 1221, in _response_cost_calculator\n    response_cost = litellm.response_cost_calculator(\n        **response_cost_calculator_kwargs\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1062, in response_cost_calculator\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1046, in response_cost_calculator\n    response_cost = completion_cost(\n        completion_response=response_object,\n    ...<10 lines>...\n        litellm_logging_obj=litellm_logging_obj,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 960, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 953, in completion_cost\n    raise e\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 915, in completion_cost\n    ) = cost_per_token(\n        ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<14 lines>...\n        rerank_billed_units=rerank_billed_units,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/cost_calculator.py", line 335, in cost_per_token\n    return openai_cost_per_token(model=model, usage=usage_block)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/llms/openai/cost_calculation.py", line 33, in cost_per_token\n    return generic_cost_per_token(\n        model=model, usage=usage, custom_llm_provider="openai"\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py", line 207, in generic_cost_per_token\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4920, in get_model_info\n    _model_info = _get_model_info_helper(\n        model=model,\n        custom_llm_provider=custom_llm_provider,\n    )\n  File "/Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/litellm/utils.py", line 4838, in _get_model_info_helper\n    raise Exception(\n    ...<3 lines>...\n    )\nException: This model isn\'t mapped yet. model=qwen-plus, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n', 'model': 'qwen-plus', 'cache_hit': None, 'custom_llm_provider': 'openai', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}
[92m11:23:12 - LiteLLM:DEBUG[0m: utils.py:4620 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
DEBUG    LiteLLM:utils.py:4620 checking potential_model_names in litellm.model_cost: {'split_model': 'qwen-plus', 'combined_model_name': 'openai/qwen-plus', 'stripped_model_name': 'qwen-plus', 'combined_stripped_model_name': 'openai/qwen-plus', 'custom_llm_provider': 'openai'}
[92m11:23:12 - LiteLLM:DEBUG[0m: utils.py:4835 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
DEBUG    LiteLLM:utils.py:4835 Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m11:23:12 - LiteLLM:DEBUG[0m: litellm_logging.py:3956 - Model=qwen-plus is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload
DEBUG    LiteLLM:litellm_logging.py:3956 Model=qwen-plus is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Crew Completion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                                                                                                                                                                                                    â”‚
â”‚  Crew Execution Completed                                                                                                                                                                                          â”‚
â”‚  Name: crew                                                                                                                                                                                                        â”‚
â”‚  ID: 4fcd4c94-a9ba-46bf-b4d6-d48be869346c                                                                                                                                                                          â”‚
â”‚  Tool Args:                                                                                                                                                                                                        â”‚
â”‚  Final Output: hello from agentbay                                                                                                                                                                                 â”‚
â”‚                                                                                                                                                                                                                    â”‚
â”‚                                                                                                                                                                                                                    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

PASSED

=============================== warnings summary ===============================
src/agentbay_sdk/tests/test_agentbay_code_flow.py::test_run_python_code_flow
src/agentbay_sdk/tests/test_agentbay_code_flow.py::test_run_python_code_flow
  /Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:
    PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Thought:...ields={'refusal': None}), input_type=Message])
    PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])
    return self.__pydantic_serializer__.to_python(

src/agentbay_sdk/tests/test_agentbay_code_flow.py::test_run_python_code_flow
src/agentbay_sdk/tests/test_agentbay_code_flow.py::test_run_python_code_flow
src/agentbay_sdk/tests/test_agentbay_code_flow.py::test_run_python_code_flow
src/agentbay_sdk/tests/test_agentbay_code_flow.py::test_run_python_code_flow
src/agentbay_sdk/tests/test_agentbay_code_flow.py::test_run_python_code_flow
src/agentbay_sdk/tests/test_agentbay_code_flow.py::test_run_python_code_flow
  /Users/chenxiangting/Workspace/crewAI-examples/crewai_env/lib/python3.13/site-packages/alibabacloud_tea_openapi/utils.py:263: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    return datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
======================== 1 passed, 8 warnings in 14.33s ========================
